##pyspark

// load data
path = "file:///home/icoder/data/SparkBasic/NYPD_7_Major_Felony_Incidents.csv"
data = sc.textFile(path)

// show some data
data.take(10)

// remove header column
header = data.first()
dataWoHeader = data.filter(lambda x: x<>header)

// Parse the rows to extract fields
dataWoHeader.map(lambda x:x.split(",")).take(10)

import csv
from StringIO import StringIO
from collections import namedtuple

fields = header.replace(" ","_").replace("/","_").split(",")

Crime = namedtuple('Crime', fields, verbose=True)

def parse(row):
    reader = csv.reader(StringIO(row))
    row = reader.next()
    return Crime(*row)

crimes = dataWoHeader.map(parse)

crimes.map(lambda x:x.Offense).countByValue()

crimesFiltered=crimes.filter(lambda x : not (x.Offense=='NA' or x.Occurrence_Year=='')).filter(lambda x : int(x.Occurrence_Year)>=2006)
crimesFiltered.map(lambda x:x.Occurrence_Year).countByValue()


def extractCoords(location):
    location_lat = float(location[1:location.index(",")])
    location_lon = float(location[location.index(",")+1:-1])
    return (location_lat,location_lon)

// calc the min location:
crimesFiltered.map(lambda x:extractCoords(x.Location_1)).reduce(lambda x,y:(min(x[0],y[0]),min(x[1],y[1])))

// calc the max location:
crimesFiltered.map(lambda x:extractCoords(x.Location_1)).reduce(lambda x,y:(max(x[0],y[0]),max(x[1],y[1])))

crimesFinal = crimesFiltered.filter(lambda x: extractCoords(x.Location_1)[0] >= 40.477399 and \
                                    extractCoords(x.Location_1)[0]<=40.917577 and \
                                    extractCoords(x.Location_1)[1]>=-74.25909 and \
                                    extractCoords(x.Location_1)[1]<=-73.700009)




















